import torch
from pathlib import Path

models_dir = Path('.')
model_files = ['multitask_best.pth', 'multitask_quantized.pth', 'viral_classifier.pth']

print("=" * 70)
print("üîç –ü–†–û–í–ï–†–ö–ê –í–°–ï–• –ú–û–î–ï–õ–ï–ô")
print("=" * 70)

for model_file in model_files:
    model_path = models_dir / model_file

    if not model_path.exists():
        print(f"\n‚ùå {model_file} - –ù–ï –ù–ê–ô–î–ï–ù")
        continue

    print(f"\n{'=' * 70}")
    print(f"üì¶ {model_file}")
    print("=" * 70)

    try:
        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)

        # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        size_mb = model_path.stat().st_size / (1024 * 1024)
        print(f"üíæ –†–∞–∑–º–µ—Ä: {size_mb:.1f} MB")

        # –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö
        print(f"üìä –¢–∏–ø: {type(checkpoint).__name__}")

        # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        if isinstance(checkpoint, dict):
            if 'model_state_dict' in checkpoint:
                print("‚úÖ –§–æ—Ä–º–∞—Ç: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏")
                print(f"   –ö–ª—é—á–∏: {list(checkpoint.keys())}")

                if 'epoch' in checkpoint:
                    print(f"   üî¢ Epoch: {checkpoint['epoch']}")
                if 'val_loss' in checkpoint:
                    print(f"   üìâ Val Loss: {checkpoint['val_loss']:.4f}")
                if 'train_loss' in checkpoint:
                    print(f"   üìâ Train Loss: {checkpoint['train_loss']:.4f}")

                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
                print("‚úÖ –§–æ—Ä–º–∞—Ç: state_dict (—Ç–æ–ª—å–∫–æ –≤–µ—Å–∞)")
        else:
            state_dict = checkpoint
            print("‚úÖ –§–æ—Ä–º–∞—Ç: OrderedDict (—Ç–æ–ª—å–∫–æ –≤–µ—Å–∞)")

        # –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        print(f"\nüèóÔ∏è  –ê–†–•–ò–¢–ï–ö–¢–£–†–ê:")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ BERT
        bert_keys = [k for k in state_dict.keys() if k.startswith('bert.')]
        if bert_keys:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º hidden_size
            emb_weight = state_dict.get('bert.embeddings.word_embeddings.weight')
            if emb_weight is not None:
                hidden_size = emb_weight.shape[1]
                print(f"   ‚Ä¢ BERT hidden_size: {hidden_size}")

                if hidden_size == 768:
                    print(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å: RuBERT-base ‚úÖ")
                elif hidden_size == 312:
                    print(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å: RuBERT-tiny2 ‚ö†Ô∏è")

            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤
            layer_nums = set()
            for key in bert_keys:
                if 'encoder.layer.' in key:
                    layer_num = int(key.split('encoder.layer.')[1].split('.')[0])
                    layer_nums.add(layer_num)

            if layer_nums:
                num_layers = max(layer_nums) + 1
                print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ BERT: {num_layers}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤
        print(f"\nüéØ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–´:")

        # popularity_head (MultiTask)
        pop_weight = state_dict.get('popularity_head.0.weight')
        if pop_weight is not None:
            print(f"   ‚úÖ popularity_head:")
            print(f"      ‚Ä¢ Shape: {pop_weight.shape}")
            print(f"      ‚Ä¢ Mean: {pop_weight.mean():.6f}")
            print(f"      ‚Ä¢ Std: {pop_weight.std():.6f}")

        # emotion_head (MultiTask)
        emo_weight = state_dict.get('emotion_head.0.weight')
        if emo_weight is not None:
            print(f"   ‚úÖ emotion_head:")
            print(f"      ‚Ä¢ Shape: {emo_weight.shape}")
            print(f"      ‚Ä¢ Mean: {emo_weight.mean():.6f}")
            print(f"      ‚Ä¢ Std: {emo_weight.std():.6f}")

        # classifier (Single task v1)
        class_weight = state_dict.get('classifier.0.weight')
        if class_weight is not None:
            print(f"   ‚úÖ classifier (v1):")
            print(f"      ‚Ä¢ Shape: {class_weight.shape}")
            print(f"      ‚Ä¢ Mean: {class_weight.mean():.6f}")
            print(f"      ‚Ä¢ Std: {class_weight.std():.6f}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é
        quantized_keys = [k for k in state_dict.keys() if 'packed_params' in k or 'scale' in k]
        if quantized_keys:
            print(f"\n‚ö° –ö–í–ê–ù–¢–ò–ó–ê–¶–ò–Ø: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ ({len(quantized_keys)} –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ—ë–≤)")

        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        print(f"\nüéØ –û–¶–ï–ù–ö–ê:")
        if pop_weight is not None and emo_weight is not None:
            if pop_weight.shape[1] == 768:
                print(f"   ‚úÖ –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è ShortsBot (MultiTask RuBERT-base)")
            else:
                print(f"   ‚ö†Ô∏è  –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ (hidden_size != 768)")
        elif class_weight is not None:
            print(f"   ‚ùå –°—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è v1 (Single Task) - –ù–ï –ü–û–î–•–û–î–ò–¢")
        else:
            print(f"   ‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")

print(f"\n{'=' * 70}")
print("‚úÖ –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
print("=" * 70)