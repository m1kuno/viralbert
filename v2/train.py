import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import ast
from model import MultiTaskClassifier

# ========== CONFIG ==========
MODEL_PATH = "./models/rubert-base"
NUM_EMOTIONS = 28
BATCH_SIZE = 16
EPOCHS = 5
LEARNING_RATE = 2e-5
MAX_LEN = 128

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üñ•Ô∏è Device: {device}")

# ========== DATASET ==========
class MultiTaskDataset(Dataset):
    def __init__(self, df, tokenizer, max_len=128, num_emotions=28):
        self.df = df.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.num_emotions = num_emotions
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        text = str(row['text'])
        task = row['task']
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        encoding = self.tokenizer(
            text,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # –ú–µ—Ç–∫–∏
        pop_label = torch.tensor(0.0)
        emo_labels = torch.zeros(self.num_emotions)
        
        if task == 'popularity':
            pop_label = torch.tensor(float(row['label']))
        elif task == 'emotion':
            # –ü–∞—Ä—Å–∏–º —Å–ø–∏—Å–æ–∫ —ç–º–æ—Ü–∏–π
            labels_str = row['emotion_labels']
            if pd.notna(labels_str) and labels_str != 'None':
                try:
                    labels = ast.literal_eval(labels_str)
                    for label_idx in labels:
                        if label_idx < self.num_emotions:
                            emo_labels[label_idx] = 1.0
                except:
                    pass
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'task': task,
            'pop_label': pop_label,
            'emo_labels': emo_labels
        }

# ========== LOAD DATA ==========
print("üìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç...")
df = pd.read_csv("v2/multitask_train.csv")
print(f"‚úÖ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")
print(f"   –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å: {len(df[df['task']=='popularity'])}")
print(f"   –≠–º–æ—Ü–∏–∏: {len(df[df['task']=='emotion'])}")

# Split
train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)
print(f"\nTrain: {len(train_df)}, Val: {len(val_df)}")

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

train_dataset = MultiTaskDataset(train_df, tokenizer, MAX_LEN, NUM_EMOTIONS)
val_dataset = MultiTaskDataset(val_df, tokenizer, MAX_LEN, NUM_EMOTIONS)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

# ========== MODEL ==========
print("\nü§ñ –°–æ–∑–¥–∞—é –º–æ–¥–µ–ª—å...")
model = MultiTaskClassifier(MODEL_PATH, NUM_EMOTIONS)
model.freeze_bert()  # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º BERT –Ω–∞ –ø–µ—Ä–≤—ã–µ —ç–ø–æ—Ö–∏
model = model.to(device)

total_params, trainable_params = model.count_parameters()
print(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,} (–æ–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,})")

optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)

# Loss functions
criterion_pop = nn.BCEWithLogitsLoss()
criterion_emo = nn.BCEWithLogitsLoss()

# ========== TRAINING ==========
print(f"\nüèãÔ∏è –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {EPOCHS} —ç–ø–æ—Ö...\n")

best_val_loss = float('inf')

for epoch in range(EPOCHS):
    # === TRAIN ===
    model.train()
    total_loss = 0
    pop_loss_sum = 0
    emo_loss_sum = 0
    pop_count = 0
    emo_count = 0
    
    progress = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")
    
    for batch in progress:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        tasks = batch['task']
        
        optimizer.zero_grad()
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –±–∞—Ç—á –ø–æ –∑–∞–¥–∞—á–∞–º
        pop_mask = torch.tensor([t == 'popularity' for t in tasks])
        emo_mask = torch.tensor([t == 'emotion' for t in tasks])
        
        loss = torch.tensor(0.0).to(device)
        batch_loss_pop = 0
        batch_loss_emo = 0
        
        # –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å
        if pop_mask.any():
            pop_indices = pop_mask.to(device)
            pop_out, _ = model(
                input_ids[pop_indices], 
                attention_mask[pop_indices], 
                task='popularity'
            )
            pop_labels = batch['pop_label'][pop_indices].to(device)
            loss_pop = criterion_pop(pop_out.squeeze(-1), pop_labels)
            loss = loss + loss_pop
            batch_loss_pop = loss_pop.item()
            pop_loss_sum += batch_loss_pop
            pop_count += 1
        
        # –≠–º–æ—Ü–∏–∏
        if emo_mask.any():
            emo_indices = emo_mask.to(device)
            _, emo_out = model(
                input_ids[emo_indices], 
                attention_mask[emo_indices], 
                task='emotion'
            )
            emo_labels = batch['emo_labels'][emo_indices].to(device)
            loss_emo = criterion_emo(emo_out, emo_labels)
            loss = loss + loss_emo
            batch_loss_emo = loss_emo.item()
            emo_loss_sum += batch_loss_emo
            emo_count += 1
        
        if loss.item() > 0:
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        progress.set_postfix({
            'loss': f'{loss.item():.4f}',
            'pop': f'{batch_loss_pop:.4f}',
            'emo': f'{batch_loss_emo:.4f}'
        })
    
    avg_loss = total_loss / len(train_loader)
    avg_pop = pop_loss_sum / max(pop_count, 1)
    avg_emo = emo_loss_sum / max(emo_count, 1)
    
    print(f"\nEpoch {epoch+1} Train - Loss: {avg_loss:.4f} (Pop: {avg_pop:.4f}, Emo: {avg_emo:.4f})")
    
    # === VALIDATION ===
    model.eval()
    val_loss = 0
    val_pop_loss = 0
    val_emo_loss = 0
    val_pop_count = 0
    val_emo_count = 0
    
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            tasks = batch['task']
            
            pop_mask = torch.tensor([t == 'popularity' for t in tasks])
            emo_mask = torch.tensor([t == 'emotion' for t in tasks])
            
            if pop_mask.any():
                pop_indices = pop_mask.to(device)
                pop_out, _ = model(input_ids[pop_indices], attention_mask[pop_indices], task='popularity')
                pop_labels = batch['pop_label'][pop_indices].to(device)
                loss_pop = criterion_pop(pop_out.squeeze(-1), pop_labels)
                val_loss += loss_pop.item()
                val_pop_loss += loss_pop.item()
                val_pop_count += 1
            
            if emo_mask.any():
                emo_indices = emo_mask.to(device)
                _, emo_out = model(input_ids[emo_indices], attention_mask[emo_indices], task='emotion')
                emo_labels = batch['emo_labels'][emo_indices].to(device)
                loss_emo = criterion_emo(emo_out, emo_labels)
                val_loss += loss_emo.item()
                val_emo_loss += loss_emo.item()
                val_emo_count += 1
    
    avg_val_loss = val_loss / len(val_loader)
    avg_val_pop = val_pop_loss / max(val_pop_count, 1)
    avg_val_emo = val_emo_loss / max(val_emo_count, 1)
    
    print(f"Epoch {epoch+1} Val   - Loss: {avg_val_loss:.4f} (Pop: {avg_val_pop:.4f}, Emo: {avg_val_emo:.4f})")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save(model.state_dict(), './models/multitask_best.pth')
        print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (val_loss: {best_val_loss:.4f})")
    
    # –ü–æ—Å–ª–µ 2 —ç–ø–æ—Ö —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤–µ—Ä—Ö–Ω–∏–µ —Å–ª–æ–∏ BERT
    if epoch == 1:
        print("\nüîì –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞—é –≤–µ—Ä—Ö–Ω–∏–µ 2 —Å–ª–æ—è BERT...")
        model.unfreeze_bert(num_layers=2)
        optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE/10)

# ========== SAVE FINAL ==========
print("\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å...")
torch.save(model.state_dict(), './models/multitask_final.pth')
print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./models/multitask_final.pth")
print(f"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: ./models/multitask_best.pth (val_loss: {best_val_loss:.4f})")
