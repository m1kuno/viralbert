import torch
import torch.nn as nn
from transformers import AutoModel

class MultiTaskClassifier(nn.Module):
    def __init__(self, model_path, num_emotions=28, hidden_size=768):
        super().__init__()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º RuBERT-base
        self.bert = AutoModel.from_pretrained(model_path)
        
        # Head 1: –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å (binary classification)
        self.popularity_head = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1)  # sigmoid –±—É–¥–µ—Ç –≤ loss
        )
        
        # Head 2: –≠–º–æ—Ü–∏–∏ (multi-label classification)
        self.emotion_head = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_emotions)  # sigmoid –±—É–¥–µ—Ç –≤ loss
        )
    
    def forward(self, input_ids, attention_mask, task='both'):
        """
        Args:
            input_ids: —Ç–æ–∫–µ–Ω—ã —Ç–µ–∫—Å—Ç–∞
            attention_mask: –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è
            task: 'popularity', 'emotion', –∏–ª–∏ 'both'
        
        Returns:
            (pop_out, emo_out) - –æ–¥–∏–Ω –∏–∑ –Ω–∏—Ö –º–æ–∂–µ—Ç –±—ã—Ç—å None
        """
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç BERT
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token
        
        pop_out = None
        emo_out = None
        
        if task in ['popularity', 'both']:
            pop_out = self.popularity_head(cls_embedding)
        
        if task in ['emotion', 'both']:
            emo_out = self.emotion_head(cls_embedding)
        
        return pop_out, emo_out
    
    def freeze_bert(self):
        """–ó–∞–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å–µ –≤–µ—Å–∞ BERT (–¥–ª—è –ø–µ—Ä–≤—ã—Ö —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è)"""
        for param in self.bert.parameters():
            param.requires_grad = False
        print("üîí BERT –∑–∞–º–æ—Ä–æ–∂–µ–Ω")
    
    def unfreeze_bert(self, num_layers=2):
        """–†–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å –≤–µ—Ä—Ö–Ω–∏–µ N —Å–ª–æ—ë–≤ BERT –¥–ª—è fine-tuning"""
        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N encoder layers
        layers = self.bert.encoder.layer
        for layer in layers[-num_layers:]:
            for param in layer.parameters():
                param.requires_grad = True
        print(f"üîì –†–∞–∑–º–æ—Ä–æ–∂–µ–Ω–æ –≤–µ—Ä—Ö–Ω–∏—Ö {num_layers} —Å–ª–æ—ë–≤ BERT")
    
    def count_parameters(self):
        """–°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        total = sum(p.numel() for p in self.parameters())
        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)
        return total, trainable


if __name__ == "__main__":
    # –¢–µ—Å—Ç –º–æ–¥–µ–ª–∏
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏...")
    
    model_path = "./models/rubert-base"
    model = MultiTaskClassifier(model_path, num_emotions=28)
    model.freeze_bert()
    
    total, trainable = model.count_parameters()
    print(f"üìä –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total:,}")
    print(f"üìä –û–±—É—á–∞–µ–º—ã—Ö: {trainable:,}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–π forward pass
    batch_size = 4
    seq_len = 128
    
    dummy_input_ids = torch.randint(0, 30000, (batch_size, seq_len))
    dummy_attention_mask = torch.ones(batch_size, seq_len)
    
    with torch.no_grad():
        pop_out, emo_out = model(dummy_input_ids, dummy_attention_mask, task='both')
    
    print(f"‚úÖ Popularity output shape: {pop_out.shape}")
    print(f"‚úÖ Emotion output shape: {emo_out.shape}")
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")
